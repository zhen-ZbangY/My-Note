## 环境说明
### 软件环境

- kubernetes v1.35
- docker 20.10.24
### 系统环境

- centos stream 10



## 1 环境初始化
所有步骤如无特殊说明，则默认所有机器执行
### 1.1 配置静态IP

- centos stream 10

  ```
  [root@k8s-master-1 ~]# cd  /etc/NetworkManager/system-connections/
  [root@k8s-master-1 system-connections]# ls
  ens160.nmconnection
  [root@k8s-master-1 system-connections]# vim ens160.nmconnection 
  [connection]
  id=ens160
  uuid=9e7c8036-4bec-3b1b-a3d1-225b9656fd81
  type=ethernet
  autoconnect-priority=-999
  interface-name=ens160
  timestamp=1770045174
  
  [ethernet]
  
  [ipv4]
  method=manual
  address1=192.168.10.171/24  #其他的机器修改成另外的ip地址
  gateway=192.168.10.2
  dns=114.114.114.114
  
  
  [ipv6]
  addr-gen-mode=eui64
  method=auto
  
  [proxy]
  [root@k8s-master-1 system-connections]# reboot  #重启验证ip地址是否配置生效
  
  查看效果
  [root@k8s-master-1 ~]# ip route  查看网关（查看路由表）
  default via 192.168.10.2 dev ens160 proto static metric 100 
  192.168.10.0/24 dev ens160 proto kernel scope link src 192.168.10.169 metric 100 
  [root@k8s-master-1 ~]# cat /etc/resolv.conf   查看dns服务器地址
  # Generated by NetworkManager
  nameserver 114.114.114.114
  [root@k8s-master-1 ~]# ip add  查看ip地址
  1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
      link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
      inet 127.0.0.1/8 scope host lo
         valid_lft forever preferred_lft forever
      inet6 ::1/128 scope host noprefixroute 
         valid_lft forever preferred_lft forever
  2: ens160: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP group default qlen 1000
      link/ether 00:0c:29:08:9b:b8 brd ff:ff:ff:ff:ff:ff
      altname enp3s0
      altname enx000c29089bb8
      inet 192.168.10.169/24 brd 192.168.10.255 scope global noprefixroute ens160
         valid_lft forever preferred_lft forever
      inet6 fe80::20c:29ff:fe08:9bb8/64 scope link noprefixroute 
         valid_lft forever preferred_lft forever
  [root@k8s-master-1 ~]# 
  
  
  ```

  

### 1.2 设置机器主机名

```shell
# 在master上执行
hostnamectl set-hostname k8s-master-1 && su
# 在node-1上执行
hostnamectl set-hostname k8s-node-1 && su
# 在node-2上执行
hostnamectl set-hostname k8s-node-2 && su
```
### 1.3 配置hosts
配置hosts文件，通过主机名互相访问

每台宿主机都要完成，在3台机器上都操作

```shell
cat >> /etc/hosts << EOF
192.168.10.172  k8s-master-1
192.168.10.173  k8s-node-1
192.168.10.174  k8s-node-2
EOF
```
### 1.4 关闭selinux
```shell
# 临时关闭
setenforce 0
# 永久关闭
sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config
# 修改selinux配置文件之后，重启机器，selinux配置才能永久生效
reboot

# 检查
getenforce
# 显示Disabled说明selinux已经关闭
```
### 1.5 禁用firewalld和iptables
```shell
# Centos
systemctl stop firewalld && systemctl disable firewalld
```
### 1.6 关闭交换分区
```shell
# 临时关闭
swapoff -a
# 永久关闭
sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
为什么要关闭交换分区？因为交换分区是使用磁盘里的空间做内存使用的，防止内存不足的时候，让容器在交换分区里运行，磁盘的运行速度比较慢，会影响到k8s集群里启动的容器应用程序的性能，所以要关闭。
```
### 1.7 调整内核参数
```shell
# 修改linux的内核参数，添加网桥过滤和地址转发功能，转发IPv4并让iptables看到桥接流量
cat <<EOF | tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF
# 加载网桥过滤模块
modprobe overlay
modprobe br_netfilter
# 编辑/etc/sysctl.d/kubernetes.conf文件，主要是对容器虚拟网络的支持，添加如下配置:
cat << EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF
# 应用sysctl参数而不重新启动
sysctl -p

# 查看br_netfilter和 overlay模块是否加载成功
lsmod | grep -e br_netfilter -e overlay
# br_netfilter           22256  0 
# bridge                151336  1 br_netfilter
# overlay                91659  0

# 查看br_netfilter和 overlay模块是否加载成功
[root@k8s-master-1 ~]# lsmod | egrep  "br_netfilter|overlay"
br_netfilter           22256  0 
overlay                91659  0 
bridge                151336  1 br_netfilter
[root@k8s-master-1 ~]# 
```
### 1.8 更新和配置软件源

* centos stream 10 

  ```
  直接可以使用官方的源，不需要配置
  ```

  

### 1.9 配置 ipvs 功能 

ipvs是一个负载均衡的软件，目前已经直接安装在linux内核里了，不需要额外安装

- centos stream 10

  ```
  # 安装ipset和ipvsadm，主要是对ipvs进行传递参数的或者管理的
  yum install ipset ipvsadm  -y
  
  # 添加需要加载的模块写入脚本文件
  #modprobe 作用是加载模块到内核里
  cat <<EOF > /etc/modules-load.d/ipvs.conf
  #!/bin/bash
  modprobe -- ip_vs
  modprobe -- ip_vs_rr
  modprobe -- ip_vs_wrr
  modprobe -- ip_vs_sh
  modprobe -- nf_conntrack
  EOF
  # 为脚本文件添加执行权限
  chmod +x /etc/modules-load.d/ipvs.conf 
  # 执行脚本文件
  /bin/bash /etc/modules-load.d/ipvs.conf 
  
  [root@k8s-master-1 ~]# vim /etc/rc.local 
  /bin/bash /etc/modules-load.d/ipvs.conf
  确保ipvs.conf里的配置会在系统重启的时候加载
  [root@k8s-master-1 ~]# chmod +x /etc/rc.d/rc.local
  
  # 重启
  reboot
  
  # 查看对应的模块是否加载成功
  [root@k8s-master-1 ~]# lsmod | egrep -e ip_vs -e nf_conntrack
  ip_vs_sh               12288  0
  ip_vs_wrr              12288  0
  ip_vs_rr               12288  0
  ip_vs                 237568  6 ip_vs_rr,ip_vs_sh,ip_vs_wrr
  nf_conntrack          229376  3 nf_nat,nft_ct,ip_vs
  nf_defrag_ipv6         24576  2 nf_conntrack,ip_vs
  nf_defrag_ipv4         12288  1 nf_conntrack
  libcrc32c              12288  5 nf_conntrack,nf_nat,nf_tables,xfs,ip_vs
  [root@k8s-master-1 ~]# 
  
  
  ```
  
  

### 1.10 配置时间同步

- Centos stream 10
  - chrony是centos里的时间管理服务，用来和其他的时间服务器同步时间
```shell
systemctl restart chronyd && systemctl enable chronyd
```

## 2 配置containerd环境
### 2.1 安装containerd环境

- centos stream 10

  ```
  第2步： 配置安装docker的源，使用docker官方提供的源
  [root@localhost ~]#sudo dnf -y install dnf-plugins-core  yum-utils 
  默认使用官方的源，在安装的时候出错，下载不下来软件，建议使用阿里云的源
  
  [root@localhost yum.repos.d]# sudo yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
  
  第3步： 安装containerd
  [root@localhost ~]#sudo dnf install  containerd.io-2.2.1  -y
  因为没有指定docker的版本，默认会安装最新的版本
  
  [root@k8s-master-1 log]# containerd --version
  containerd containerd.io v2.2.1 dea7da592f5d1d2b7755e3a161be07f43fad8f75
  [root@k8s-master-1 log]# 
  
  
  ```
  
  

### 2.2 配置containerd的国内源

```shell
# 1. 生成默认配置
containerd config default > /etc/containerd/config.toml
# 2. 核心修改：开启systemd cgroup驱动（K8s强制要求）+ 替换pause镜像为国内源
sed -i 's/SystemdCgroup \= false/SystemdCgroup \= true/g' /etc/containerd/config.toml
sed -i 's/registry.k8s.io\/pause:3.10/registry.aliyuncs.com\/google_containers\/pause:3.10/g' /etc/containerd/config.toml
# 3. 配置国内镜像仓库（加速+解决镜像拉取失败）
cat >> /etc/containerd/config.toml << EOF
[plugins."io.containerd.grpc.v1.cri".registry.mirrors]
  [plugins."io.containerd.grpc.v1.cri".registry.mirrors."docker.io"]
    endpoint = ["https://hub-mirror.c.163.com", "https://mirror.baidubce.com"]
  [plugins."io.containerd.grpc.v1.cri".registry.mirrors."k8s.gcr.io"]
    endpoint = ["https://registry.aliyuncs.com/google_containers"]
  [plugins."io.containerd.grpc.v1.cri".registry.mirrors."quay.io"]
    endpoint = ["https://registry.aliyuncs.com/calico"]
EOF

# 验证配置：输出SystemdCgroup = true则成功
grep -n "SystemdCgroup" /etc/containerd/config.toml
```
### 2.3 配置containerd服务自启动
```shell

systemctl restart containerd && systemctl enable containerd
# 验证状态（active (running)则成功）
systemctl status containerd

```
## 2.4 检查/etc/hosts文件的配置，测试网络连通性

每台机器的/etc/hosts,添加下面的配置

```
[root@k8s-master-1 docker]# cat /etc/hosts 
# Loopback entries; do not change.
# For historical reasons, localhost precedes localhost.localdomain:
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
# See hosts(5) for proper format and other examples:
# 192.168.1.10 foo.example.org foo
# 192.168.1.13 bar.example.org bar
192.168.10.172  k8s-master-1
192.168.10.173  k8s-node-1
192.168.10.174  k8s-node-2
[root@k8s-master-1 docker]# 
```

在node2机器上进行测试，通过访问域名是否可以ping通其他的机器

```
[root@k8s-master-1 docker]# cat /etc/hosts 
# Loopback entries; do not change.
# For historical reasons, localhost precedes localhost.localdomain:
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
# See hosts(5) for proper format and other examples:
# 192.168.1.10 foo.example.org foo
# 192.168.1.13 bar.example.org bar
192.168.10.172  k8s-master-1
192.168.10.173  k8s-node-1
192.168.10.174  k8s-node-2
[root@k8s-master-1 docker]# 
#测试能否ping通其他主机和上互联网
[root@k8s-master-1 containerd]# ping k8s-master-1
PING k8s-master-1 (192.168.10.172) 56(84) bytes of data.
64 bytes from k8s-master-1 (192.168.10.172): icmp_seq=1 ttl=64 time=0.045 ms
^C
--- k8s-master-1 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.045/0.045/0.045/0.000 ms
[root@k8s-master-1 containerd]# ping k8s-node-1
PING k8s-node-1 (192.168.10.173) 56(84) bytes of data.
64 bytes from k8s-node-1 (192.168.10.173): icmp_seq=1 ttl=64 time=0.739 ms
^C
--- k8s-node-1 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.739/0.739/0.739/0.000 ms
[root@k8s-master-1 containerd]# ping k8s-node-2
PING k8s-node-2 (192.168.10.174) 56(84) bytes of data.
64 bytes from k8s-node-2 (192.168.10.174): icmp_seq=1 ttl=64 time=1.36 ms
64 bytes from k8s-node-2 (192.168.10.174): icmp_seq=2 ttl=64 time=1.27 ms
^C
--- k8s-node-2 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1006ms
rtt min/avg/max/mdev = 1.266/1.313/1.361/0.047 ms
[root@k8s-master-1 containerd]# ping www.jd.com
PING wwwv6.jcloudimg.com (175.12.124.3) 56(84) bytes of data.
64 bytes from 175.12.124.3: icmp_seq=1 ttl=128 time=10.7 ms
64 bytes from 175.12.124.3: icmp_seq=2 ttl=128 time=12.7 ms
^C
--- wwwv6.jcloudimg.com ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1009ms
rtt min/avg/max/mdev = 10.737/11.718/12.699/0.981 ms
[root@k8s-master-1 containerd]# 

```

此处建议在虚拟机里创建一个快照，containerd已经安装好了



## 3 配置k8s集群环境

### 3.1 配置k8s组件源

- Centos  每台节点服务器都需要执行下面的命令（master和node节点）
- centos stream 10
```shell
#高版本的k8s的阿里云的源
cat <<EOF | tee /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.35/rpm/
enabled=1
gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.35/rpm/repodata/repomd.xml.key
EOF


# 更新索引缓冲
yum makecache
```

### 3.2 安装

- Centos
  - kubeadm 是用来安装master节点上的组件（apiserver、etcd、scheduler等），部署k8s集群的。
  - kubelet 是用来对容器运行时软件进行管理的（管理docker和containerd等容器运行时软件的）
  - kubectl 是用来输入k8s的命令的，是k8s给用户使用的软件，是一个客户端命令
````shell
# 安装kubeadm、kubelet、kubectl v1.35.0
dnf install -y kubelet-1.35.0 kubeadm-1.35.0 kubectl-1.35.0 --disableexcludes=kubernetes

# 验证安装版本（均为v1.35.0则成功）
kubeadm version && kubelet --version && kubectl version --client

#设置 kubelet 开机自启（kubelet 会由 kubeadm 自动启动，先配置自启）
systemctl enable kubelet
# 此时kubelet为inactive状态，属于正常现象
systemctl status kubelet

````

### 3.3 拉取 K8s 集群所需镜像（Master 节点初始化 K8s 集群 ，仅在 k8s-master 执行）

仅在 k8s-master 执行

kubeadm 初始化时会自动拉取镜像，国内网络需指定**阿里云镜像仓库**，执行以下命令提前拉取（也可初始化时指定）：

```shell
kubeadm config images pull \
--image-repository registry.aliyuncs.com/google_containers \
--kubernetes-version v1.35.0
# 验证镜像（containerd镜像列表，包含apiserver、controller-manager等则成功）
#crictl 是k8s里查看镜像的工具--》类似于docker命令
crictl images

#具体效果如下
[root@k8s-master-1 kubelet]# kubeadm config images pull \
--image-repository registry.aliyuncs.com/google_containers \
--kubernetes-version v1.35.0
# 验证镜像（containerd镜像列表，包含apiserver、controller-manager等则成功）
crictl images
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-apiserver:v1.35.0
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-controller-manager:v1.35.0
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-scheduler:v1.35.0
[config/images] Pulled registry.aliyuncs.com/google_containers/kube-proxy:v1.35.0
[config/images] Pulled registry.aliyuncs.com/google_containers/coredns:v1.13.1
[config/images] Pulled registry.aliyuncs.com/google_containers/pause:3.10.1
[config/images] Pulled registry.aliyuncs.com/google_containers/etcd:3.6.6-0
WARN[0000] Config "/etc/crictl.yaml" does not exist, trying next: "/usr/bin/crictl.yaml" 
WARN[0000] Image connect using default endpoints: [unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock]. As the default settings are now deprecated, you should set the endpoint instead. 
IMAGE                                                             TAG                 IMAGE ID            SIZE
registry.aliyuncs.com/google_containers/coredns                   v1.13.1             aa5e3ebc0dfed       23.6MB
registry.aliyuncs.com/google_containers/etcd                      3.6.6-0             0a108f7189562       23.6MB
registry.aliyuncs.com/google_containers/kube-apiserver            v1.35.0             5c6acd67e9cd1       27.7MB
registry.aliyuncs.com/google_containers/kube-controller-manager   v1.35.0             2c9a4b058bd7e       23.1MB
registry.aliyuncs.com/google_containers/kube-proxy                v1.35.0             32652ff1bbe6b       25.8MB
registry.aliyuncs.com/google_containers/kube-scheduler            v1.35.0             550794e3b12ac       17.2MB
registry.aliyuncs.com/google_containers/pause                     3.10.1              cd073f4c5f6a8       320kB
[root@k8s-master-1 kubelet]# 
说明：crictl是k8s里命令行的工具，用于替代 docker 命令操作 K8s 容器。
```
## 4 集群初始化
### 4.1 kubeadm init
仅在master节点执行
```shell

kubeadm init \
--apiserver-advertise-address=192.168.1.10 `# master节点实际IP` \
--kubernetes-version=v1.35.0 `# K8s版本` \
--image-repository=registry.aliyuncs.com/google_containers `# 国内镜像源` \
--service-cidr=10.96.0.0/12 `# K8s Service网段（固定，不要修改）` \
--pod-network-cidr=10.244.0.0/16 `# Pod网段，与后续calico网络插件配置一致` \
--ignore-preflight-errors=all `# 忽略预检错误（如内存/CPU略低的情况）`

kubeadm init \
--apiserver-advertise-address=192.168.10.172  \
--kubernetes-version=v1.35.0  \
--image-repository=registry.aliyuncs.com/google_containers  \
--service-cidr=10.96.0.0/12  \
--pod-network-cidr=10.244.0.0/16  \
--ignore-preflight-errors=all
```
成功后会提示以下信息：
```shell
[root@k8s-master-1 kubelet]# kubeadm init \
--apiserver-advertise-address=192.168.10.169  \
--kubernetes-version=v1.35.0  \
--image-repository=registry.aliyuncs.com/google_containers  \
--service-cidr=10.96.0.0/12  \
--pod-network-cidr=10.244.0.0/16  \
--ignore-preflight-errors=all
[init] Using Kubernetes version: v1.35.0
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [k8s-master-1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.10.169]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [k8s-master-1 localhost] and IPs [192.168.10.169 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [k8s-master-1 localhost] and IPs [192.168.10.169 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "super-admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/instance-config.yaml"
[patches] Applied patch of type "application/strategic-merge-patch+json" to target "kubeletconfiguration"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
[kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
[kubelet-check] The kubelet is healthy after 510.095502ms
[control-plane-check] Waiting for healthy control plane components. This can take up to 4m0s
[control-plane-check] Checking kube-apiserver at https://192.168.10.169:6443/livez
[control-plane-check] Checking kube-controller-manager at https://127.0.0.1:10257/healthz
[control-plane-check] Checking kube-scheduler at https://127.0.0.1:10259/livez
[control-plane-check] kube-controller-manager is healthy after 502.561312ms
[control-plane-check] kube-scheduler is healthy after 1.481000956s
[control-plane-check] kube-apiserver is healthy after 3.0048236s
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Skipping phase. Please see --upload-certs
[mark-control-plane] Marking the node k8s-master-1 as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node k8s-master-1 as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule]
[bootstrap-token] Using token: 1zl0x4.w1ji9rp0wzvgf7n6
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.10.169:6443 --token 1zl0x4.w1ji9rp0wzvgf7n6 \
	--discovery-token-ca-cert-hash sha256:9fa1c77ab31bce8850bc2ed87bb96023ee684ec8325e462c5494058f83c68f33 
## 暂存这条命令


kubeadm join 192.168.10.169:6443 --token 1zl0x4.w1ji9rp0wzvgf7n6 \
	--discovery-token-ca-cert-hash sha256:9fa1c77ab31bce8850bc2ed87bb96023ee684ec8325e462c5494058f83c68f33  # 暂存这条命令
```
然后在master节点上继续执行
```shell
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
#查看master接口监听6443端口的进程，是kube-apiserver
[root@k8s-master-1 ~]# ss -anplut|grep 6443
tcp    LISTEN     0      128    [::]:6443               [::]:*                   users:(("kube-apiserver",pid=2809,fd=7))
[root@k8s-master-1 ~]# 



```
### 4.2 所有的node节点加入集群
```shell
# 在所有的node节点上面执行的命令
kubeadm join 192.168.10.169:6443 --token 1zl0x4.w1ji9rp0wzvgf7n6 \
	--discovery-token-ca-cert-hash sha256:9fa1c77ab31bce8850bc2ed87bb96023ee684ec8325e462c5494058f83c68f33 

        
#执行下面的命令也可以得到join集群的命令，如果忘记了或者没有提前复制保存
[root@k8s-master-1 ~]# kubeadm token create --print-join-command
kubeadm join 192.168.205.140:6443 --token yvq3xz.x0xfu68vz4rqldvw --discovery-token-ca-cert-hash sha256:1afc3cdd9442dfb8e1d6420c2cdb551a994e3972dbad5468193735b9ea18c086 
[root@k8s-master-1 ~]# 
```
如果成功，检查集群节点状态
```shell
# 在master上执行
[root@k8s-master-1 kubelet]# kubectl get node
NAME           STATUS     ROLES           AGE     VERSION
k8s-master-1   NotReady   control-plane   2m59s   v1.35.0
k8s-node-1     NotReady   <none>          20s     v1.35.0
k8s-node-2     NotReady   <none>          11s     v1.35.0
[root@k8s-master-1 kubelet]# 
```
分配worker role
```shell
# 在master上执行
kubectl label node k8s-node-1 node-role.kubernetes.io/worker=worker
kubectl label node k8s-node-2 node-role.kubernetes.io/worker=worker


```
在node节点上也可以执行kubectl命令

```
[root@k8s-node-1 ~]# mkdir -p $HOME/.kube
[root@k8s-node-1 ~]# scp  k8s-master-1:/etc/kubernetes/admin.conf /root/.kube/config   
[root@k8s-node-1 ~]# chown $(id -u):$(id -g) $HOME/.kube/config

[root@k8s-node-1 kubelet]# kubectl get node
NAME           STATUS     ROLES           AGE     VERSION
k8s-master-1   NotReady   control-plane   8m49s   v1.35.0
k8s-node-1     NotReady   worker          6m10s   v1.35.0
k8s-node-2     NotReady   worker          6m1s    v1.35.0
[root@k8s-node-1 kubelet]# 
```



### 4.3 安装Calico网络插件

安装 Calico 网络插件（仅在 master 节点执行，解决 Pod 网络互通）

K8s 集群初始化后，节点状态为`NotReady`，因为缺少网络插件，**必须安装网络插件后，Pod 和节点才能互通**。本次选择 Calico（官方推荐，支持网络策略，适配大规模集群）。

calico是k8s集群里网络通信的一个组件（软件），实现集群内部不同的机器之间的容器的通信（大的网络规模）。k8s集群节点可以到5000节点，1亿个容器

flannel 也是一个网络插件（适合小规模的集群，节点服务器的数量比较小，例如：1~500个节点）

terway 是阿里云自己研发的一个网络插件

cilium 是网络插件，实现集群内部不同的机器之间的容器的通信（大的网络规模）。

```shell
# master执行

# 下载Calico v3.28.0配置文件（与K8s v1.35兼容）
yum  install wget -y
wget https://raw.githubusercontent.com/projectcalico/calico/v3.28.0/manifests/calico.yaml -O calico.yaml
# 替换Calico镜像为国内源（解决拉取慢）
#sed -i 's/docker.io\/calico/registry.aliyuncs.com\/calico/g' calico.yaml
# 验证Pod网段（与kubeadm init的--pod-network-cidr=10.244.0.0/16一致，无需修改）
[root@k8s-master-1 ~]# grep -C 2 -E 'podCIDR|CALICO_IPV4POOL_CIDR' calico.yaml
            # chosen from this range. Changing this value after installation will have
            # no effect. This should fall within `--cluster-cidr`.
            - name: CALICO_IPV4POOL_CIDR
              value: "192.168.0.0/16"
            # Disable file logging so `kubectl logs` works.
[root@k8s-master-1 ~]# 
#默认是value: "192.168.0.0/16" 修改为--pod-network-cidr 保持一致10.244.0.0/16
#下面进行修改
[root@k8s-master-1 ~]# ls
anaconda-ks.cfg  calico.yaml
[root@k8s-master-1 ~]#
[root@k8s-master-1 ~]# vim calico.yaml 

    - name: CALICO_IPV4POOL_CIDR
      value: "10.244.0.0/16"   #修改为--pod-network-cidr 保持一致


#安装calico插件
kubectl apply -f calico.yaml
[root@k8s-master-1 calico]# kubectl get pod -n kube-system
NAME                                       READY   STATUS                  RESTARTS      AGE
calico-kube-controllers-79fcb48598-7nwck   0/1     Pending                 0             43s
calico-node-5dpmk                          0/1     Init:ImagePullBackOff   0             43s
calico-node-jdkgf                          0/1     Init:ImagePullBackOff   0             43s
calico-node-lzqmj                          0/1     Init:ImagePullBackOff   0             43s
coredns-bbdc5fdf6-2r9fp                    0/1     Pending                 0             62m
coredns-bbdc5fdf6-nsfmd                    0/1     Pending                 0             62m
etcd-k8s-master-1                          1/1     Running                 1 (15m ago)   62m
kube-apiserver-k8s-master-1                1/1     Running                 1 (15m ago)   62m
kube-controller-manager-k8s-master-1       1/1     Running                 1 (15m ago)   62m
kube-proxy-6gp9c                           1/1     Running                 1 (15m ago)   57m
kube-proxy-s4fqq                           1/1     Running                 1 (15m ago)   57m
kube-proxy-zfmrx                           1/1     Running                 1 (15m ago)   62m
kube-scheduler-k8s-master-1                1/1     Running                 1 (15m ago)   62m
[root@k8s-master-1 calico]# 

 kubectl describe  pod  calico-node-5dpmk -n kube-system

[root@k8s-master-1 calico]# kubectl get pod -n kube-system -o wide
NAME                                       READY   STATUS                  RESTARTS      AGE   IP               NODE           NOMINATED NODE   READINESS GATES
calico-kube-controllers-79fcb48598-7nwck   0/1     Pending                 0             24m   <none>           <none>         <none>           <none>
calico-node-5dpmk                          0/1     Init:ImagePullBackOff   0             24m   192.168.10.173   k8s-node-1     <none>           <none>
calico-node-jdkgf                          0/1     Init:ImagePullBackOff   0             24m   192.168.10.172   k8s-master-1   <none>           <none>
calico-node-lzqmj                          0/1     Init:ImagePullBackOff   0             24m   192.168.10.174   k8s-node-2     <none>           <none>
coredns-bbdc5fdf6-2r9fp                    0/1     Pending                 0             85m   <none>           <none>         <none>           <none>
coredns-bbdc5fdf6-nsfmd                    0/1     Pending                 0             85m   <none>           <none>         <none>           <none>
etcd-k8s-master-1                          1/1     Running                 1 (38m ago)   85m   192.168.10.172   k8s-master-1   <none>           <none>
kube-apiserver-k8s-master-1                1/1     Running                 1 (38m ago)   85m   192.168.10.172   k8s-master-1   <none>           <none>
kube-controller-manager-k8s-master-1       1/1     Running                 1 (38m ago)   85m   192.168.10.172   k8s-master-1   <none>           <none>
kube-proxy-6gp9c                           1/1     Running                 1 (38m ago)   80m   192.168.10.173   k8s-node-1     <none>           <none>
kube-proxy-s4fqq                           1/1     Running                 1 (38m ago)   80m   192.168.10.174   k8s-node-2     <none>           <none>
kube-proxy-zfmrx                           1/1     Running                 1 (38m ago)   85m   192.168.10.172   k8s-master-1   <none>           <none>
kube-scheduler-k8s-master-1                1/1     Running                 1 (38m ago)   85m   192.168.10.172   k8s-master-1   <none>           <none>
[root@k8s-master-1 calico]# 

#查看启动不了的pod的事件，发现是镜像下载不了，导致pod启动失败
kubectl describe  pod  calico-node-5dpmk -n kube-system
Events:
  Type     Reason     Age                   From               Message
  ----     ------     ----                  ----               -------
  Normal   Scheduled  25m                   default-scheduler  Successfully assigned kube-system/calico-node-5dpmk to k8s-node-1
  Warning  Failed     24m                   kubelet            spec.initContainers{upgrade-ipam}: Failed to pull image "docker.io/calico/cni:v3.28.0": failed to pull and unpack image "docker.io/calico/cni:v3.28.0": failed to resolve image: failed to do request: Head "https://registry-1.docker.io/v2/calico/cni/manifests/v3.28.0": dial tcp 74.86.118.24:443: connect: connection refused
  Warning  Failed     24m                   kubelet            spec.initContainers{upgrade-ipam}: Failed to pull image "docker.io/calico/cni:v3.28.0": failed to pull and unpack image "docker.io/calico/cni:v3.28.0": failed to resolve image: failed to do request: Head "https://registry-1.docker.io/v2/calico/cni/manifests/v3.28.0": dial tcp 199.59.150.12:443: connect: connection refused
  Normal   Pulling    21m (x5 over 25m)     kubelet            spec.initContainers{upgrade-ipam}: Pulling image "docker.io/calico/cni:v3.28.0"
  Warning  Failed     20m (x5 over 24m)     kubelet            spec.initContainers{upgrade-ipam}: Error: ErrImagePull
  Warning  Failed     20m (x3 over 23m)     kubelet            spec.initContainers{upgrade-ipam}: Failed to pull image "docker.io/calico/cni:v3.28.0": failed to pull and unpack image "docker.io/calico/cni:v3.28.0": failed to resolve image: failed to do request: Head "https://registry-1.docker.io/v2/calico/cni/manifests/v3.28.0": dial tcp 47.88.58.234:443: connect: connection refused
  Warning  Failed     4m45s (x77 over 24m)  kubelet            spec.initContainers{upgrade-ipam}: Error: ImagePullBackOff
  Normal   BackOff    12s (x95 over 24m)    kubelet            spec.initContainers{upgrade-ipam}: Back-off pulling image "docker.io/calico/cni:v3.28.0"
[root@k8s-master-1 calico]#
```

### 4.4 应用 Calico 配置文件



```
先去渡渡鸟里下载containerd的镜像

ctr images pull swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/calico/node:v3.28.0
ctr images tag  swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/calico/node:v3.28.0  docker.io/calico/node:v3.28.0

ctr images pull swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/calico/cni:v3.28.0
ctr images tag  swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/calico/cni:v3.28.0  docker.io/calico/cni:v3.28.0

ctr images pull swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/calico/kube-controllers:v3.28.0
ctr images tag  swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/calico/kube-controllers:v3.28.0  docker.io/calico/kube-controllers:v3.28.0

ctr images pull swr.cn-north-4.myhuaweicloud.com/ddn-k8s/registry.k8s.io/pause:3.10.1
ctr images tag  swr.cn-north-4.myhuaweicloud.com/ddn-k8s/registry.k8s.io/pause:3.10.1  registry.k8s.io/pause:3.10.1


[root@k8s-master-1 ~]# ctr images ls|grep calico
docker.io/calico/cni:v3.28.0                                                       application/vnd.docker.distribution.manifest.v2+json sha256:ffaf92254159406599968af9cd16d994f7af1fab02c55f0609597dda64d79b56 84.1 MiB  linux/arm64 -      
docker.io/calico/kube-controllers:v3.28.0                                          application/vnd.docker.distribution.manifest.v2+json sha256:206926f6f12f01eab89dd5f8f1e2b33b7b8f2174a85e7dab69cd9008d6062f0f 33.3 MiB  linux/amd64 -      
docker.io/calico/node:v3.28.0                                                      application/vnd.docker.distribution.manifest.v2+json sha256:3c0e24adecc39e89780e807400def972deb0ec9de9fbbbaceade072a8c6ae94f 109.9 MiB linux/amd64 -      
k8s.io/calico/node:v3.28.0                                                         application/vnd.docker.distribution.manifest.v2+json sha256:3c0e24adecc39e89780e807400def972deb0ec9de9fbbbaceade072a8c6ae94f 109.9 MiB linux/amd64 -      
swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/calico/cni:v3.28.0-linuxarm64   application/vnd.docker.distribution.manifest.v2+json sha256:ffaf92254159406599968af9cd16d994f7af1fab02c55f0609597dda64d79b56 84.1 MiB  linux/arm64 -      
swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/calico/kube-controllers:v3.28.0 application/vnd.docker.distribution.manifest.v2+json sha256:206926f6f12f01eab89dd5f8f1e2b33b7b8f2174a85e7dab69cd9008d6062f0f 33.3 MiB  linux/amd64 -      
swr.cn-north-4.myhuaweicloud.com/ddn-k8s/docker.io/calico/node:v3.28.0             application/vnd.docker.distribution.manifest.v2+json sha256:3c0e24adecc39e89780e807400def972deb0ec9de9fbbbaceade072a8c6ae94f 109.9 MiB linux/amd64 -      
[root@k8s-master-1 ~]# 



导出calico相关的4个镜像

ctr images export calico-cni-v3.28.0.tar docker.io/calico/cni:v3.28.0
ctr images export kube-controllers-v3.28.0.tar docker.io/calico/kube-controllers:v3.28.0
ctr images export node-v3.28.0.tar docker.io/calico/node:v3.28.0
ctr images export k8s.io.pause-3.10.1.tar registry.k8s.io/pause:3.10.1
导入镜像到k8s.io命名空间里
[root@k8s-master-1 ~]# ctr -n k8s.io images import calico-cni-v3.28.0.tar
[root@k8s-master-1 ~]# ctr -n k8s.io images import kube-controllers-v3.28.0.tar 
[root@k8s-master-1 ~]# ctr -n k8s.io images import node-v3.28.0.tar 
[root@k8s-master-1 ~]# ctr -n k8s.io images import k8s.io.pause-3.10.1.tar

再次查看是否导入成功
[root@k8s-master-1 ~]# ctr -n k8s.io images ls

再次查看是否导入成功
[root@k8s-master-1 ~]# crictl images
IMAGE                                                             TAG                 IMAGE ID            SIZE
docker.io/calico/cni                                              v3.28.0             adcb19ea66141       88.2MB
docker.io/calico/kube-controllers                                 v3.28.0             428d92b022539       35MB
docker.io/calico/node                                             v3.28.0             4e42b6f329bc1       115MB
registry.aliyuncs.com/google_containers/coredns                   v1.13.1             aa5e3ebc0dfed       23.6MB
registry.aliyuncs.com/google_containers/etcd                      3.6.6-0             0a108f7189562       23.6MB
registry.aliyuncs.com/google_containers/kube-apiserver            v1.35.0             5c6acd67e9cd1       27.7MB
registry.aliyuncs.com/google_containers/kube-controller-manager   v1.35.0             2c9a4b058bd7e       23.1MB
registry.aliyuncs.com/google_containers/kube-proxy                v1.35.0             32652ff1bbe6b       25.8MB
registry.aliyuncs.com/google_containers/kube-scheduler            v1.35.0             550794e3b12ac       17.2MB
registry.aliyuncs.com/google_containers/pause                     3.10.1              cd073f4c5f6a8       320kB
[root@k8s-master-1 ~]# 

#使用v3.25版本的calico网络插件
kubectl apply -f https://docs.projectcalico.org/archive/v3.25/manifests/calico.yaml

#
kubectl apply -f calico.yaml
# 查看Calico Pod状态（所有Pod为Running则成功，约1-3分钟）
kubectl get pods -n kube-system -w  #按ctrl+c退出
[root@k8s-master-1 ~]# kubectl get pod -n kube-system -o wide  #查看pod的状态

#calico安装成功的效果
[root@k8s-master-1 ~]# kubectl get node   查看节点的状态都是ready
NAME           STATUS   ROLES           AGE    VERSION
k8s-master-1   Ready    control-plane   125m   v1.35.0
k8s-node-1     Ready    worker          120m   v1.35.0
k8s-node-2     Ready    worker          120m   v1.35.0
[root@k8s-master-1 ~]#
#查看kube-system命名空间里的pod
[root@k8s-master-1 ~]# kubectl get pod -n kube-system -o wide
NAME                                       READY   STATUS    RESTARTS      AGE    IP               NODE           NOMINATED NODE   READINESS GATES
calico-kube-controllers-79fcb48598-gsgqf   1/1     Running   0             33m    10.244.109.66    k8s-node-1     <none>           <none>
calico-node-52dbf                          1/1     Running   0             33m    192.168.10.173   k8s-node-1     <none>           <none>
calico-node-fgpl2                          1/1     Running   0             33m    192.168.10.174   k8s-node-2     <none>           <none>
calico-node-wngxc                          1/1     Running   0             33m    192.168.10.172   k8s-master-1   <none>           <none>
coredns-bbdc5fdf6-2r9fp                    1/1     Running   0             123m   10.244.109.65    k8s-node-1     <none>           <none>
coredns-bbdc5fdf6-nsfmd                    1/1     Running   0             123m   10.244.109.67    k8s-node-1     <none>           <none>
etcd-k8s-master-1                          1/1     Running   1 (76m ago)   123m   192.168.10.172   k8s-master-1   <none>           <none>
kube-apiserver-k8s-master-1                1/1     Running   1 (76m ago)   123m   192.168.10.172   k8s-master-1   <none>           <none>
kube-controller-manager-k8s-master-1       1/1     Running   1 (76m ago)   123m   192.168.10.172   k8s-master-1   <none>           <none>
kube-proxy-6gp9c                           1/1     Running   1 (76m ago)   118m   192.168.10.173   k8s-node-1     <none>           <none>
kube-proxy-s4fqq                           1/1     Running   1 (76m ago)   117m   192.168.10.174   k8s-node-2     <none>           <none>
kube-proxy-zfmrx                           1/1     Running   1 (76m ago)   123m   192.168.10.172   k8s-master-1   <none>           <none>
kube-scheduler-k8s-master-1                1/1     Running   1 (76m ago)   123m   192.168.10.172   k8s-master-1   <none>           <none>
[root@k8s-master-1 ~]# 

```



```
# 创建crictl配置文件，指定containerd的运行时套接字
cat > /etc/crictl.yaml << EOF
runtime-endpoint: unix:///var/run/containerd/containerd.sock
image-endpoint: unix:///var/run/containerd/containerd.sock
timeout: 10
debug: false
pull-image-on-create: true
EOF

# 验证配置，无警告即成功
crictl info


[root@k8s-master-1 ~]# crictl images
IMAGE                                                             TAG                 IMAGE ID            SIZE
docker.io/calico/cni                                              v3.28.0             adcb19ea66141       88.2MB
docker.io/calico/kube-controllers                                 v3.28.0             428d92b022539       35MB
docker.io/calico/node                                             v3.28.0             4e42b6f329bc1       115MB
registry.aliyuncs.com/google_containers/coredns                   v1.13.1             aa5e3ebc0dfed       23.6MB
registry.aliyuncs.com/google_containers/etcd                      3.6.6-0             0a108f7189562       23.6MB
registry.aliyuncs.com/google_containers/kube-apiserver            v1.35.0             5c6acd67e9cd1       27.7MB
registry.aliyuncs.com/google_containers/kube-controller-manager   v1.35.0             2c9a4b058bd7e       23.1MB
registry.aliyuncs.com/google_containers/kube-proxy                v1.35.0             32652ff1bbe6b       25.8MB
registry.aliyuncs.com/google_containers/kube-scheduler            v1.35.0             550794e3b12ac       17.2MB
registry.aliyuncs.com/google_containers/pause                     3.10.1              cd073f4c5f6a8       320kB
[root@k8s-master-1 ~]# 
      

```



### 4.5 k8s配置ipvs

ipvs 是linux里的一个负载均衡软件，默认在linux内核里就安装了，修改k8s里的一个配置，负载均衡的时候使用ipvs做为默认的负载均衡软件，如果不修改默认是iptables

```shell
kubectl edit configmap kube-proxy -n kube-system
# 修改配置
mode: "ipvs"

# 删除所有kube-proxy pod使之重启
kubectl delete pods -n kube-system -l k8s-app=kube-proxy
```
![image-20240112100835525.png](https://cdn.nlark.com/yuque/0/2024/png/35578695/1708682207799-d89cc978-8401-4081-9a10-0f943f4e19e8.png#averageHue=%23262322&clientId=u06adfa1a-1bad-4&from=ui&id=ucc3e3d11&originHeight=333&originWidth=369&originalType=binary&ratio=1&rotation=0&showTitle=false&size=30762&status=done&style=none&taskId=ud17cac08-c04b-40cd-b349-c2a99ef7da1&title=)
> GPU调度不配置IPVS会报错：nvidia-container-cli: device error: unknown device id: no-gpu-has-10MiB-to-run 无法分配GPU内存
> 根本原因是POD调度与自定义GPU调度器通信问题

## 5 常用软件
### 5.1 安装Dashboard
以下命令均只在master节点上执行

![image-20240805111758488](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20240805111758488.png)

#### 下载安装
```shell
yum  install  wget -y
wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml
# 修改Service部分，改为NodePort对外暴露端口
```
![2024-01-21_16-38-24.png](https://cdn.nlark.com/yuque/0/2024/png/35578695/1708682234341-70ce6dee-71f9-48d0-af64-a57cd7a7fbba.png#averageHue=%23272220&clientId=u06adfa1a-1bad-4&from=ui&id=ue8d48e2b&originHeight=358&originWidth=364&originalType=binary&ratio=1&rotation=0&showTitle=false&size=32318&status=done&style=none&taskId=u057a5525-7a1d-4d10-a9dc-08f29c2e53a&title=)<br />安装
```shell
kubectl apply -f recommended.yaml
```
#### 查看
```shell
kubectl get pods,svc -n kubernetes-dashboard

NAME                                             READY   STATUS    RESTARTS   AGE
pod/dashboard-metrics-scraper-5657497c4c-x8srr   1/1     Running   0          3d22h
pod/kubernetes-dashboard-78f87ddfc-2b6rq         1/1     Running   0          3d22h

NAME                                TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGE
service/dashboard-metrics-scraper   ClusterIP   10.109.73.125    <none>        8000/TCP        3d22h
service/kubernetes-dashboard        NodePort    10.101.254.225   <none>        443:30088/TCP   3d22h
```
#### 创建账号
创建dashboard-access-token.yaml文件
```yaml
# Creating a Service Account
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kubernetes-dashboard
---
# Creating a ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: admin-user
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: admin-user
  namespace: kubernetes-dashboard
---
# Getting a long-lived Bearer Token for ServiceAccount
apiVersion: v1
kind: Secret
metadata:
  name: admin-user
  namespace: kubernetes-dashboard
  annotations:
    kubernetes.io/service-account.name: "admin-user"
type: kubernetes.io/service-account-token
# Clean up and next steps
# kubectl -n kubernetes-dashboard delete serviceaccount admin-user
# kubectl -n kubernetes-dashboard delete clusterrolebinding admin-user
```
执行
```shell
kubectl apply -f dashboard-access-token.yaml
# 获取token
kubectl get secret admin-user -n kubernetes-dashboard -o jsonpath={".data.token"} | base64 -d
```
#### 访问dashboard
```shell
# 获取端口
kubectl get svc -n kubernetes-dashboard
NAME                        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGE
dashboard-metrics-scraper   ClusterIP   10.109.73.125    <none>        8000/TCP        3d23h
kubernetes-dashboard        NodePort    10.101.254.225   <none>        443:30088/TCP   3d23h  # 端口为30088
```
浏览器访问：`https://ClusterIP:PORT`  注意https<br />页面中输入上一步获取到的token即可

```
解决token默认15分钟过期的问题
[root@k8s-master-1 ~]# vim recommended.yaml 


193       containers:
194         - name: kubernetes-dashboard
195           image: kubernetesui/dashboard:v2.7.0
196           imagePullPolicy: Always
197           ports:
198             - containerPort: 8443
199               protocol: TCP
200           args:
201             - --auto-generate-certificates
202             - --namespace=kubernetes-dashboard
203             - --token-ttl=43200         #添加这条配置，超时时间调整为12小时
重新应用
[root@k8s-master-1 ~]# kubectl apply  -f recommended.yaml 

```

![img](file:///C:\Users\Administrator\Documents\Tencent Files\695811769\nt_qq\nt_data\Pic\2025-09\Ori\55c8bb06ebc96ffe580b8636cd88f5d9.png)

![img](file:///C:\Users\Administrator\Documents\Tencent Files\695811769\nt_qq\nt_data\Pic\2025-09\Ori\a6792c67fd7a2e6bd55852795e1caffc.png)

### 5.2 安装Kuborad

任意节点执行，推荐master

kuboard 是一个多k8s集群的管理软件，假如公司内部有多套k8s集群，可以使用kuboard来进行管理。

```shell
sudo docker run -d \
  --restart=unless-stopped \
  --name=kuboard \
  -p 80:80/tcp \  # 可根据需要修改第一个暴露的port
  -p 10081:10081/tcp \  # 无特殊需要不建议修改
  -e KUBOARD_ENDPOINT="http://IP:PORT" \  # 部署在哪台机器就用什么IP:PORT
  -e KUBOARD_AGENT_SERVER_TCP_PORT="10081" \
  -v /root/kuboard-data:/data \  # 可根据需要修改第一个数据挂载路径
  swr.cn-east-2.myhuaweicloud.com/kuboard/kuboard:v3
  
 #冯老师上课的例子
  sudo docker run -d \
  --restart=unless-stopped \
  --name=kuboard \
  -p 80:80/tcp \
  -p 10081:10081/tcp \
  -e KUBOARD_ENDPOINT="http://192.168.205.141:10081" \
  -e KUBOARD_AGENT_SERVER_TCP_PORT="10081" \
  -v /kuboard-data:/data \
  swr.cn-east-2.myhuaweicloud.com/kuboard/kuboard:v3
  
  访问的是master节点的http://192.168.205.141

```
浏览器访问 `http://IP:PORT`

- 用户名： `admin`
- 密 码： `Kuboard123`
### 5.3 安装kubectl命令自动补全
```shell
yum install -y bash-completion

# 临时设置自动补全
source <(kubectl completion bash) 
# 永久设置自动补全
echo "source <(kubectl completion bash)" >> ~/.bashrc && bash
```
### 5.4 部署metric-server
> 使用Kuboard可一键集成，无需手动部署
>
> ![image-20240805150808300](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20240805150808300.png)

#### 下载
master执行
```yaml
wget https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.6.2/components.yaml
```
#### 修改
`vim`修改140行左右<br />原：

```yaml
containers:
- args:
  ...
  image: k8s.gcr.io/metrics-server/metrics-server:v0.6.2
```
修改后：
```yaml
containers:
- args:
  ...
  - --kubelet-insecure-tls  # 添加这一行
  image: registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server:v0.6.2  # 修改镜像仓库地址
```
#### 应用
```shell
kubectl apply -f components.yaml
```
#### 查看
```shell
[root@k8s-master ~]# kubectl top nodes
NAME         CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%   
k8s-master   276m         6%     3445Mi          44%       
k8s-node-1   202m         0%     11326Mi         17%       
k8s-node-2   204m         0%     14497Mi         22%   

[root@k8s-master kubernetes]# kubectl top pods
NAME                                        CPU(cores)   MEMORY(bytes)   
nginx-test-d5db944-6wqkw                    0m           14Mi            
rpa-llm-embedding-service-9477877dc-fzg88   1m           492Mi
```
### 5.5 kubectl inspect拓展
```shell
# 下载kubectl
curl -LO https://storage.googleapis.com/kubernetes-release/release/v1.23.17/bin/linux/amd64/kubectl
chmod +x ./kubectl
sudo mv ./kubectl /usr/bin/kubectl
# 下载kubectl拓展
cd /usr/bin/
wget https://mirror.ghproxy.com/https://github.com/AliyunContainerService/gpushare-device-plugin/releases/download/v0.3.0/kubectl-inspect-gpushare
chmod u+x /usr/bin/kubectl-inspect-gpushare
# 使用
kubectl inspect gpushare
NAME            IPADDRESS     GPU0(Allocated/Total)  GPU1(Allocated/Total)  GPU2(Allocated/Total)  GPU Memory(GiB)
k8s-poc-node-3  172.19.7.101  16/24                  8/24                   0/24                   24/72
----------------------------------------------------------------------
Allocated/Total GPU Memory In Cluster:
24/72 (33%)  
# 宽展开
kubectl inspect gpushare -d
```
### 5.6 部署nfs
#### 服务器
```shell
# CentOS安装
yum -y install nfs-utils rpcbind
# Ubuntu安装
apt install -y nfs-kernel-server

# 创建文件共享目录
vim /etc/exports

/opt/nfs  172.168.1.0/24(rw,no_root_squash,no_all_squash,sync)
# 共享目录 共享IP的网段(共享权限)
# 可以使用*允许所有IP网段挂载

# 使NFS配置生效
exportfs -r

# CentOS启动NFS服务
systemctl enable --now nfs-server
# Ubuntu启动NFS服务
service nfs-kernel-server start && service nfs-kernel-server enable

# 查看本机共享的路径
showmount -e 127.0.0.1
```
> 权限参数说明<br />ro :该主机有只读权限<br />rw :该主机对共享目录有可读可写的权限<br />all_squash ：任何用户访问服务器都是匿名用户访问，相当于使用nobody用户访问该共享目录。<br />no_all_squash :和all_squash相反，该选项默认设置<br />root_squash :将root用户及所属组都映射为匿名用户或用户组，为默认设置<br />no_root_squash ：root用户具有对根目录的完全管理访问权限<br />no_subtree_check：不检查父目录的权限<br />anonuid :将远程访问的所有用户都映射为匿名用户，并指定该用户为本地用户<br />anongid :将远程访问的所有用户组都映射为匿名用户组账户，并指定该匿名用户组账户为本地用户组账户<br />sync :将数据同步写入内存缓冲区与磁盘中，效率底，但可以保证数据的一致性<br />async :将数据先保存在内存缓冲区中，必要时才写入磁盘

#### 客户端
```shell
# Centos安装
yum -y install rpcbind
# Ubuntu安装
apt install -y nfs-common

# 挂载
mount -t nfs -o rw nolock ip:/opt/nfs /opt/nfs

# 取消挂载
umount /opt/nfs
# 如果服务器NFS服务宕机，而客户端正在挂载使用时，客户端会出现df -h命令卡死。这时需要强制取消挂载
umount -lf /opt/nfs
```
### 5.7 安装Krew 和Kube-Capacity
#### 安装插件管理器Krew

K8s（Kubernetes）中的Krew是一个插件管理工具，专为kubectl命令行工具设计。它类似于系统包管理工具（如apt、dnf或brew，yum），使得用户可以轻松地搜索、安装、卸载和管理kubectl插件。Krew的存在极大地扩展了kubectl的功能，允许用户通过安装社区提供的各种插件来增强kubectl的能力。

```shell
# 下载压缩包
wget  https://mirror.ghproxy.com/https://github.com/kubernetes-sigs/krew/releases/latest/download/krew-linux_amd64.tar.gz

# 解压
tar -zxvf krew-linux_amd64.tar.gz

# 配置环境变量
vim  ~/.bashrc
....
export PATH="${PATH}:${HOME}/.krew/bin"
.....
# 重新bash
bash

# 验证
./krew-linux_amd64 version

OPTION            VALUE
GitTag            v0.4.4
GitCommit         343e657
IndexURI          https://github.com/kubernetes-sigs/krew-index.git
BasePath          /root/.krew
IndexPath         /root/.krew/index/default
InstallPath       /root/.krew/store
BinPath           /root/.krew/bin
DetectedPlatform  linux/amd64

# 安装
mv ./krew-linux_amd64 ./kubectl-krew
mv ./kubectl-krew /usr/local/bin/
# 验证
kubectl krew version

yum install git -y
# 更新索引
kubectl krew update
kubectl krew info

# 使用
kubectl krew search
NAME                            DESCRIPTION                                         INSTALLED
access-matrix                   Show an RBAC access matrix for server resources     no
accurate                        Manage Accurate, a multi-tenancy controller         no
advise-policy                   Suggests PodSecurityPolicies and OPA Policies f...  no
...
```
#### 安装资源使用量终端管理工具resource-capacity
> resource-capacity依赖metrics-server获取数据

```shell
# 这一步可能失败 需要从GitHub下载文件 多试几次
kubectl krew install resource-capacity
```
#### 使用resource-capacity

- 查看节点情况
```shell
kubectl resource-capacity
NODE         CPU REQUESTS   CPU LIMITS      MEMORY REQUESTS   MEMORY LIMITS
*            27568m (34%)   52560m (65%)    54516Mi (27%)     57008Mi (28%)
k8s-master   1212m (30%)    270m (6%)       570Mi (7%)        390Mi (5%)
k8s-node-1   2248m (28%)    3460m (43%)     3788Mi (11%)      3708Mi (11%)
...
```

- 查看POD
```shell
kubectl resource-capacity --pods

NODE         NAMESPACE       POD                                                       CPU REQUESTS   CPU LIMITS      MEMORY REQUESTS   MEMORY LIMITS
*            *               *                                                         27568m (34%)   52560m (65%)    54516Mi (27%)     57008Mi (28%)

k8s-master   *               *                                                         1212m (30%)    270m (6%)       570Mi (7%)        390Mi (5%)
k8s-master   kube-system     calico-kube-controllers-64cc74d646-f5z2t                  0m (0%)        0m (0%)         0Mi (0%)          0Mi (0%)
...
```

- 宽输出
```shell
kubectl resource-capacity --pods --util

NODE         NAMESPACE       POD                                                       CPU REQUESTS   CPU LIMITS      CPU UTIL     MEMORY REQUESTS   MEMORY LIMITS   MEMORY UTIL
*            *               *                                                         27568m (34%)   52560m (65%)    1303m (1%)   54516Mi (27%)     57008Mi (28%)   57538Mi (28%)

k8s-master   *               *                                                         1212m (30%)    270m (6%)       316m (7%)    570Mi (7%)        390Mi (5%)      3401Mi (44%)
k8s-master   kube-system     calico-kube-controllers-64cc74d646-f5z2t                  0m (0%)        0m (0%)         4m (0%)      0Mi (0%)          0Mi (0%)        21Mi (0%)
...
```
> 来自 pod 的利用率数字可能不会与总节点利用率相加。
> 与节点和集群级别数字代表 pod 值总和的请求和限制数字不同，节点指标直接来自指标服务器，并且可能包括其他形式的资源利用率。

- 排序
```shell
kubectl resource-capacity --util --sort cpu.util

[root@k8s-master ~]# kubectl resource-capacity --util --sort cpu.util
NODE         CPU REQUESTS   CPU LIMITS      CPU UTIL     MEMORY REQUESTS   MEMORY LIMITS   MEMORY UTIL
*            27568m (34%)   52560m (65%)    1416m (1%)   54516Mi (27%)     57008Mi (28%)   57583Mi (28%)
k8s-master   1212m (30%)    270m (6%)       315m (7%)    570Mi (7%)        390Mi (5%)      3403Mi (44%)
k8s-node-1   2248m (28%)    3460m (43%)     280m (3%)    3788Mi (11%)      3708Mi (11%)    15599Mi (48%)
...
```

- 显示 Pod 计数
```shell
kubectl resource-capacity --pod-count

NODE         CPU REQUESTS   CPU LIMITS      MEMORY REQUESTS   MEMORY LIMITS   POD COUNT
*            27568m (34%)   52560m (65%)    54516Mi (27%)     57008Mi (28%)   69/770
k8s-master   1212m (30%)    270m (6%)       570Mi (7%)        390Mi (5%)      13/110
k8s-node-1   2248m (28%)    3460m (43%)     3788Mi (11%)      3708Mi (11%)    17/110
...
```

- 标签过滤
```shell
kube-capacity --pod-labels app=nginx  
kube-capacity --namespace 默认
kube-capacity --namespace-labels team=api  
kube-capacity --node-labels kubernetes.io/role=node
```
## 6 常见问题
### 6.1 k8s端口限制
> 非必要不推荐修改

#### 问题描述
创建Service时，小于30000的端口时会提示端口限制：<br />`The Service "envirment" is invalid: spec.ports[0].nodePort: Invalid value: 8098: provided port is not in the valid range. The range of valid ports is 30000-32767`
#### 解决方案
编辑api-server配置：`vim /etc/kubernetes/manifests/kube-apiserver.yaml`<br />40行左右
```yaml
spec:
  containers:
  - command:   
    ...
    - --service-cluster-ip-range=10.96.0.0/12
    - --service-node-port-range=1-65535  # 添加这一行
```
完成后重启kubelet，`systemctl restart kubelet`
### 6.2 kubeadm join忘记token/token过期
完整命令：`kubeadm join masterIP:6443 --token xxx --discovery-token-ca-cert-hash sha256:xxx`<br />在master执行，获取token和discovery-token-ca-cert-hash
```shell
# 获取 token 参数
# 查看已有token
kubeadm token list
TOKEN     TTL         EXPIRES                USAGES                   DESCRIPTION                            
aaa.aaa   5h          2024-01-12T07:38:58Z   authentication,signing   The default bootstrap token generated by 'kubeadm init'. 
# 没有token则执行，创建新的 TOKEN并打印 join 语句
kubeadm token create --print-join-command

# 获取 discovery-token-ca-cert-hash 参数 
openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null |  openssl dgst -sha256 -hex | sed 's/^.* //'

# 替换token参数和cert-hash参数
kubeadm join masterIP:6443 --token aaa.aaa --discovery-token-ca-cert-hash sha256:xxxxx
```
## 7 配置GPU调度
### 7.1 master
#### 部署GPU共享调度扩展器
```shell
kubectl create -f https://mirror.ghproxy.com/https://raw.githubusercontent.com/AliyunContainerService/gpushare-scheduler-extender/master/config/gpushare-schd-extender.yaml
```
#### 下载调度文件
```
cd /etc/kubernetes
curl -O https://mirror.ghproxy.com/https://raw.githubusercontent.com/AliyunContainerService/gpushare-scheduler-extender/master/config/scheduler-policy-config.yaml
# 修改配置文件： 127.0.0.1 => master内网IP
```
#### 在调度参数中添加策略配置文件参数
```shell
vim /etc/kubernetes/manifests/kube-scheduler.yaml
# 添加以下内容
- --config=/etc/kubernetes/scheduler-policy-config.yaml
```
![image-20240104134026003.png](https://cdn.nlark.com/yuque/0/2024/png/35578695/1708682429563-8ce9f6d4-90fe-485c-b95a-2b940485873e.png#averageHue=%23252422&clientId=u06adfa1a-1bad-4&from=ui&id=ua1996552&originHeight=230&originWidth=674&originalType=binary&ratio=1&rotation=0&showTitle=false&size=41084&status=done&style=none&taskId=ua74996f1-9736-44f4-933f-ed0ddf64891&title=)
#### 添加卷挂载
```shell
# 添加以下内容
- mountPath: /etc/kubernetes/scheduler-policy-config.yaml
  name: scheduler-policy-config
  readOnly: true
  
- hostPath:
      path: /etc/kubernetes/scheduler-policy-config.yaml
      type: FileOrCreate
  name: scheduler-policy-config
```
![image-20240104134154749.png](https://cdn.nlark.com/yuque/0/2024/png/35578695/1708682447439-acbb6e81-3adb-49b5-aa3c-52665c39aec5.png#averageHue=%23242321&clientId=u06adfa1a-1bad-4&from=ui&id=u9488e6d8&originHeight=164&originWidth=560&originalType=binary&ratio=1&rotation=0&showTitle=false&size=28230&status=done&style=none&taskId=u984c4f1a-b6e6-466a-a2a1-45a55c30a13&title=)<br />![image-20240104134221771.png](https://cdn.nlark.com/yuque/0/2024/png/35578695/1708682453705-7d7de875-5a9c-4fbc-9765-4ffcd2669793.png#averageHue=%23242321&clientId=u06adfa1a-1bad-4&from=ui&id=ud267e7cf&originHeight=210&originWidth=522&originalType=binary&ratio=1&rotation=0&showTitle=false&size=30662&status=done&style=none&taskId=ub54f3915-ec66-44af-8ffa-861eb251bf4&title=)
#### 部署设备插件
```shell
kubectl create -f https://mirror.ghproxy.com/https://raw.githubusercontent.com/AliyunContainerService/gpushare-device-plugin/master/device-plugin-rbac.yaml
kubectl create -f https://mirror.ghproxy.com/https://raw.githubusercontent.com/AliyunContainerService/gpushare-device-plugin/master/device-plugin-ds.yaml
```
#### 为需要GPU共享的节点添加gpushare节点标签
```shell
kubectl label node <target_node> gpushare=true
```
### 7.2 node
#### 确保已安装NVIDIA驱动
通过`nvidia-smi`检查
#### 安装nvidia-docker2

- Centos
```shell
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.repo | sudo tee /etc/yum.repos.d/nvidia-docker.repo

yum update
yum install -y nvidia-docker2
pkill -SIGHUP dockerd
```

- Ubuntu
```shell
distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \
      && curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \
      && curl -s -L https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | \
            sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
            sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list

apt update
apt install -y nvidia-docker2
```
#### 设置NVIDIA runtime为Docker默认运行时环境
```shell
vim /etc/docker/daemon.json

{
    ...  # 原有配置
    "default-runtime": "nvidia",
    "runtimes": {
        "nvidia": {
           "path": "/usr/bin/nvidia-container-runtime",
           "runtimeArgs": []
      }
   }
}
```
#### 重新加载配置
```shell
pkill -SIGHUP dockerd
systemctl daemon-reload
systemctl restart docker
# 验证
docker info | grep  "Default Runtime"
# Default Runtime: nvidia
```
#### 分配GPU资源
设置limit：aliyun.com/gpu-mem: "2"  单位Gi<br />![image-20240111152718738.png](https://cdn.nlark.com/yuque/0/2024/png/35578695/1708682508236-1d0621e9-d181-4fc5-92a5-c0f96fc28021.png#averageHue=%23f3f3f3&clientId=u06adfa1a-1bad-4&from=ui&id=uef398fbe&originHeight=274&originWidth=445&originalType=binary&ratio=1&rotation=0&showTitle=false&size=13820&status=done&style=none&taskId=u6f54704c-45c4-4492-8d66-3a0d58a106b&title=)
### 7.3 报错nvidia-container-cl: device error
报错：nvidia-container-cli: device error: unknown device id: no-gpu-has-10MiB-to-run 无法分配GPU内存<br />检查：

1. 是否修改配置文件 /etc/kubernetesscheduler-policy-config.yaml： 127.0.0.1 => master内网IP
2. 是否正确配置各节点IPVS，集群IPVS（参考步骤1.9和4.4）
### 7.4 Ubuntu GPU驱动大坑
Ubuntu默认自动更新内核，更新后GPU驱动会出问题，需要关闭更新
```shell
# 全部改成0
vim /etc/apt/apt.conf.d/10periodic
APT::Periodic::Update-Package-Lists "0";
APT::Periodic::Download-Upgradeable-Packages "0";
APT::Periodic::AutocleanInterval "0";

vim /etc/apt/apt.conf.d/20auto-upgrades
APT::Periodic::Update-Package-Lists "0";
APT::Periodic::Unattended-Upgrade "0";
```
